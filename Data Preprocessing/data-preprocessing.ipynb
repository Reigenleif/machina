{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv') # df = data frame, pd.read_csv = fungsi untuk membaca file csv\n",
    "\n",
    "# Dependent vars : semua baris dengan kolom 0-2\n",
    "x = df.iloc[:,:-1].values # df.iloc memilih baris dan kolom berdasarkan index, argumen pertama untuk range baris dan argumen kedua untuk range kolom\n",
    "\n",
    "# Independent vars : semua baris dengan kolom 3\n",
    "y = df.iloc[:,3].values\n",
    "\n",
    "# Note : dalam python, range menyertakan batas bawah dan tidak menyertakan batas atas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking care of missing data\n",
    "\n",
    "# Terdapat 2 cara untuk mengatasi missing data\n",
    "# 1. Menghapus baris yang mengandung missing data : tidak masalah jika data yang hilang hanya sedikit\n",
    "# 2. Mengganti missing data dengan nilai rata-rata dari kolom tersebut\n",
    "# Akan digunakan cara kedua\n",
    "\n",
    "from sklearn.impute import SimpleImputer# sklearn.preprocessing adalah library untuk preprocessing data, imputer adalah class yang digunakan untuk mengganti missing data\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean') # missing_values adalah nilai yang akan diganti, strategy adalah cara penggantian nilai (rata-rata)\n",
    "imputer.fit(x[:,1:3]) # imputer.fit digunakan untuk mengganti nilai yang hilang, argumen pertama adalah data yang akan diganti\n",
    "x[:,1:3] = imputer.transform(x[:,1:3]) # imputer.transform digunakan untuk mengganti nilai yang hilang, argumen pertama adalah data yang akan diganti\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "# Hot encoding : mengkode setiap kategori dalam suatu data kategorikal menjadi kolom baru\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# objek ct (column transformer) dibuat dengan class ColumnTransformer dengan argumen :\n",
    "# transformers : list of tuples, dimana setiap tuple berisi nama transformer, transformer, dan kolom yang akan diubah\n",
    "# remainder : 'passthrough' untuk menyertakan kolom yang tidak diubah, 'drop' untuk menghapus kolom yang tidak diubah, defaultnya adalah 'drop'\n",
    "\n",
    "ct  = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')\n",
    "\n",
    "# ct.fit_transform digunakan untuk menerapkan ct pada sebuah matriks, argumen pertama adalah matriks yang akan diubah\n",
    "# np.array digunakan untuk mengubah hasil ct.fit_transform menjadi array numpy (lebih cepat dari list biasa)\n",
    "x = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding dependent variable\n",
    "# Label encoding : mengkode setiap kategori dalam suatu data kategorikal menjadi angka\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# le.fit_transform digunakan untuk mengubah nilai pada variabel dependen y menjadi angka\n",
    "y = le.fit_transform(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "# Training set : data yang digunakan untuk melatih model\n",
    "# Test set : data yang digunakan untuk menguji model\n",
    "\n",
    "# splitting dilakukan sebelum feature scaling karena test set memerlukan dataset yang benar-benar baru\n",
    "# jika feature scaling dilakukan sebelum splitting, maka data test akan terpengaruh oleh data training, disebut juga information leakage\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_test_split digunakan untuk membagi dataset menjadi training set dan test set\n",
    "# argumen pertama adalah variabel independen, argumen kedua adalah variabel dependen \n",
    "# argumen ketiga adalah test_size, yaitu persentase data yang akan dijadikan test set\n",
    "# argumen keempat adalah random_state, yaitu seed untuk random number generator, jika tidak diisi maka akan diisi secara random\n",
    "# argumen kelima adalah shuffle, yaitu apakah data akan diacak atau tidak, defaultnya adalah True\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
      "[[0.0 1.0 0.0 -1.0 -1.0]\n",
      " [1.0 0.0 0.0 1.0 1.0]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# Feature scaling dilakukan untuk mengubah skala data menjadi sama, sehingga tidak ada data yang mendominasi data lain\n",
    "# Feature scaling dilakukan setelah splitting karena test set harus benar-benar baru\n",
    "# Terdapat 2 cara untuk melakukan feature scaling :\n",
    "# 1. Standardisation : x_stand = (x-mean(x))/std(x), works all the time (lebih disarankan)\n",
    "# 2. Normalisation : x_norm = (x-min(x))/(max(x)-min(x)), works only when data is normally distributed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # StandardScaler adalah class untuk melakukan standardisation\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train[:,3:] = sc.fit_transform(x_train[:,3:]) # sc.fit_transform digunakan untuk melakukan feature scaling, argumen pertama adalah data yang akan diubah\n",
    "x_test[:,3:] = sc.fit_transform(x_test[:,3:]) # sc.transform digunakan untuk melakukan feature scaling, argumen pertama adalah data yang akan diubah\n",
    "\n",
    "# x_train dan x_test adalah data hasil preprocessing\n",
    "print(x_train)\n",
    "print(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
